<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="LERSUS 3.3.0.0" /><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><title>Корпоративные информационные системы — Теория — Обработка распределенных запросов и распределенная модель транзакций</title><script language="javascript" type="text/javascript" src="../../00/styles/files/apiwrapper.js"> </script><script language="javascript" type="text/javascript" src="../../00/styles/files/sco.js"> </script><link href="../../00/styles/files/style.css" rel="stylesheet" type="text/css" /></head><body><table id="mainOuter"><tr><td id="topOuter"><table id="topInner"><tr class="row1"><td class="cell1"><div> </div></td><td class="cell2"><div> </div></td><td class="cell3"><div> </div></td><td class="cell4" id="shortcuts"><div><ul class="level0"><li><a href="../../00/content/authors1.html">Авторы</a></li><li>|</li><li><a href="../../00/content/contacts1.html">Контакты</a></li><li>|</li><li><a href="../../00/content/instructions1.html">Методические указания</a></li><li>|</li><li><a href="../../00/content/annotation1.html">Аннотация</a></li><li>|</li><li><a href="#" onclick="history.back(); return false;">Назад</a></li></ul></div></td></tr><tr class="row2"><td class="cell1"><div> </div></td><td class="cell2"><div> </div></td><td class="cell3"><div> </div></td><td class="cell4"><div id="courseTitle">Корпоративные информационные системы</div><div id="courseSubTitle">Распределенные базы данных : Обработка распределенных запросов и распределенная модель транзакций</div></td></tr></table></td></tr><tr><td id="contentOuter"><table id="contentInner"><tr><td class="menuOuter"><table class="menuInner"><tr><td><ul class="level0"><li><a href="../../00/styles/courses.html">Содержание курса</a></li><li><span>Теория</span></li><ul class="level1"><li><a href="../../04/content/content1.html">Распределенное хранение данных</a></li><li><a href="../../04/content/content2.html">Транзакции</a></li><li><a href="../../04/content/content3.html">Восстановление данных. Управление параллелизмом</a></li><li><a href="../../04/content/content4.html">Обработка распределенных запросов и распределенная модель транзакций</a></li><li><a href="../../04/content/content5.html">Распределение и тиражирование данных</a></li></ul><li><span>Текущий контроль знаний</span></li><ul class="level1"><li><a href="../../04/content/multiply-choice-test1.html">Транзакции и блокировки</a></li><li><a href="../../04/content/multiply-choice-test5.html">Распределенные базы данных</a></li></ul><li><a href="../../04/styles/glossary.html">Словарь терминов</a></li><li><a href="../../04/04.pdf" target="_blank">Версия для печати</a></li></ul></td></tr></table></td><td class="content"><div id="content"><div id="topBlankStripe"> </div><span id="lnkf9e3e3a8c55e47dc98dd58d95426d001"> </span><h1 align="left" class="headline-source">     Обработка распределенных запросов и распределенная модель транзакций</h1><span id="lnk43a1685e568745fa951f6238bbc90e55"> </span><h2 align="left" class="paragraph-headline-source">     Проблемы распределенных систем</h2><span id="lnk8bccc7b1c04c43ce884f93e4353e3c6f"> </span><div class="section"><p align="justify" class="paragraph-source">     Ниже более подробно описываются некоторые уже упомянутые проблемы. Основная проблема информационных сетей, по крайней мере глобальных, заключается в том, что они достаточно медленны. В типичной глобальной сети интенсивность обмена данными равна приблизительно 5 или 10 тыс. байт в секунду, а для обычного жесткого диска интенсивность обмена данными - около 5 или 10.млн байт в секунду. Вследствие этого основным требованием к распределенным системам является минимизация использования сети, т.е. сокращение до минимума количества и объема пересылаемых в сети сообщений. Стремление к достижению этой цели приводит, в свою очередь, к необходимости решения перечисленных ниже проблем:</p><p align="justify" class="paragraph-source">     - обработка запросов;</p><p align="justify" class="paragraph-source">     - управление каталогом;</p><p align="justify" class="paragraph-source">     - распространение обновления;</p><p align="justify" class="paragraph-source">     - управление восстановлением;</p><p align="justify" class="paragraph-source">     - управление параллелизмом.</p></div><span id="lnk6e3d28947e2d4809a3b9e56db116f336"> </span><h2 align="left" class="paragraph-headline-source">     Обработка запросов</h2><span id="lnk2c9a07b4314747b78f09e1e76bcd1806"> </span><div class="section"><p align="justify" class="paragraph-source">     <b>Распределенным</b> называется запрос, который обращается к двум и более узлам РБД, но не обновляет на них данные. </p><p align="justify" class="paragraph-source">     Запрашивающий узел должен определить, что в запросе идет обращение к данным на другом узле, выделить подзапрос к удаленному узлу и перенаправить его этому узлу.</p><p align="justify" class="paragraph-source">     Самой сложной проблемой выполнения распределенных запросов является <b>оптимизация</b>, т.е. поиск оптимального плана выполнения запроса. Информация, которая требуется для оптимизации запроса, распределена по узлам. Если выбрать центральный узел, который соберет эту информацию, построит оптимальный план и отправит его на выполнение, то теряется свойство локальной автономности. </p><p align="justify" class="paragraph-source">     Поэтому обычно распределенный запрос выполняется так: запрашивающий узел собирает все данные, полученные в результате выполнения подзапросов, у себя, и выполняет их соединение (или объединение), что может занять очень много времени.</p><p align="justify" class="paragraph-source">     При минимизации использования сети предполагается, что сама по себе оптимизация запроса, как и его исполнение, должна быть распределенной. Иначе говоря, общий процесс оптимизации обычно состоит из этапа глобальной оптимизации, который сопровождается несколькими этапами локальной оптимизации. </p><p align="justify" class="paragraph-source">     Например, допустим, что запрос <i>Q </i>задан на узле <i>X </i>и включает объединение отношения <i>R</i><i>у</i>, содержащего сто кортежей на узле <i>Y</i>, и отношения <i>Rz</i>, содержащего миллион кортежей на узле <i>Z </i>.</p><p align="justify" class="paragraph-source">     Оптимизатор на узле <i>X </i>выберет глобальную стратегию для выполнения запроса <i>Q</i>, при этом, очевидно, лучше переместить отношение <i>Ry </i>на узел <i>Z</i>, а не отношение <i>Rz </i>на узел <i>Y </i>(и конечно же, не следует перемещать оба отношения <i>Ry </i>и <i>Rz </i>на узел <i>X</i>. Тогда сразу после перемещения отношения <i>Ry </i>на узел <i>Z </i>стратегия выполнения объединения на узле <i>Z </i>будет выбрана локальным оптимизатором на узле <i>Z</i>.</p></div><span id="lnk3b9b973b4d5d4bf097fcff25d09e32c5"> </span><table class="note"><tr><td><p align="justify" class="note-source">     Пример: Рассмотрим базу данных поставщиков и деталей (упрощенный вариант):</p><p align="justify" class="note-source">S { S#, CITY } 10 000 хранимых кортежей на узле A</p><p align="justify" class="note-source">Р { P#, COLOR } 100 000 хранимых кортежей на узле В</p><p align="justify" class="note-source">SР { S#, P# } 1 000 000 хранимых кортежей на узле А</p><p align="justify" class="note-source">     Предполагается, что каждый хранимый кортеж имеет размер 25 байт (200 бит).</p><p align="justify" class="note-source">     Запрос "Получить сведения о находящихся в Лондоне (London) поставщиках красных (Red) деталей:</p><p align="justify" class="note-source">S.S# WHERE EXISTS SP EXISTS Р ( S.CITY = 'London' AND</p><p align="justify" class="note-source">S.S# = SP.S# AND</p><p align="justify" class="note-source">SP.P# = P.P# AND</p><p align="justify" class="note-source">P.COLOR = 'Red' )</p><p align="justify" class="note-source">     Оценочные границы промежуточных результатов:</p><p align="justify" class="note-source">     -Число красных деталей = 10</p><p align="justify" class="note-source">     -Число поставок, выполняемых поставщиками из Лондона = 100000 </p><p align="justify" class="note-source">     Предполагаемые параметры обмена данными в сети:</p><p align="justify" class="note-source">     -Интенсивность обмена данными = 50 000 бит/с</p><p align="left" class="note-source">     -Задержка доступа = 0,1 с</p></td><th><img src="../../00/styles/files/note_problem.png" title="Проблемма" alt="Проблемма" /></th></tr></table><span id="lnka8240d08ca8948758c076f494855846d"> </span><div class="section"><p align="left" class="paragraph-source">     </p><p align="justify" class="paragraph-source">     Теперь можно вкратце рассмотреть шесть возможных стратегий обработки этого запроса с вычислением для каждой <i>i</i>-стратегии общего времени передачи данных T[ <i>i </i>] по следующей формуле:</p><p align="justify" class="paragraph-source">T[<i>i</i>] <i>= </i>общая задержка доступа + (общий объем данных / интенсивность обмена данными) = (число сообщений / 10) +(число бит / 50000)</p></div><span id="lnkb0ecdb7167e84f93bdc5cb492843ee5b"> </span><div class="section"><p align="justify" class="paragraph-source">     1 шаг. Переместить отношение <i>Р </i>на узел <i>А </i>и выполнить запрос на узле <i>А</i>.</p><p align="justify" class="paragraph-source">T[ 1] = 0,1 + (100 000*200)/ 50 000 = приблизительно 400 с (6,67 мин)</p><p align="justify" class="paragraph-source">     2 шаг. Переместить отношения <i>S </i>и <i>SP </i>на узел <i>В </i>и выполнить запрос на узле <i>В</i>.</p><p align="justify" class="paragraph-source">T[ 2 ] = 0,2 + ((10 000 + 1 000 000)*200)/50 000 = приблизительно 4 040 с (1,12 ч)</p><p align="justify" class="paragraph-source">     3 шаг. Соединить отношения <i>S </i>и <i>SP </i>на узле <i>А</i>, выбрать из полученного результата кортежи для поставщиков из Лондона, а затем для каждого кортежа на узле <i>В </i>проверить, не является ли соответствующая деталь красной. Каждая из этих проверок будет содержать два сообщения: запрос и ответ. Время передачи данных для таких сообщений будет значительно меньше по сравнению с задержкой доступа.</p><p align="justify" class="paragraph-source">T[ 3 ] = приблизительно 20 000 с (5,56 ч)</p><p align="justify" class="paragraph-source">     4 шаг. Выбрать из отношения <i>Р </i>на узле <i>В </i>кортежи, соответствующие красным деталям, а затем для каждого кортежа на узле <i>А </i>проверить, не поставляется ли соответствующая деталь поставщиком из Лондона. Каждая из этих проверок будет содержать два сообщения: запрос и ответ. Время передачи данных для этих сообщений опять будет значительно меньше по сравнению с задержкой доступа.</p><p align="justify" class="paragraph-source">T[ 4 ] = приблизительно 2 с</p><p align="justify" class="paragraph-source">     5 шаг. Соединить отношения <i>S </i>и <i>SP </i>на узле <i>А</i>, выбрать из полученного результата кортежи для поставщиков из Лондона, результат разбить на проекции по атрибутам <i>S# </i>и <i>P#</i>, а затем переместить на узел <i>В</i>. Завершить выполнение запроса на узле В.</p><p align="justify" class="paragraph-source">T[ 5 ] = 0,1 + (100 000*200)/50 000 = приблизительно 400 с (6,67 мин)</p><p align="justify" class="paragraph-source">     6 шаг. Выбрать из отношения <i>Р </i>на узле <i>В </i>кортежи, соответствующие красным деталям, а затем переместить результат на узел А. Завершить выполнение запроса на узле А. </p><p align="justify" class="paragraph-source">T[ 6 ] = 0,1 + (10*200)/50 000 = приблизительно 0,1 с</p><p align="justify" class="paragraph-source">     Эти результаты представлены в таблице 19</p></div><span id="lnk08c9dbd8992946909bd5cf23bc773011"> </span><div align="center"><div class="inline-table-container"><p align="left" class="unit-title">Таблица 19 - Стратегии распределенного выполнения запроса (итоги)</p><table class="inline-table"><col width="80" /><col width="291" /><col width="188" /><tbody><tr><td style="background-color:#ffffff"><p align="left" class="inline-table">Стратегия</p></td><td style="background-color:#ffffff"><p align="left" class="inline-table">Метод</p></td><td style="background-color:#ffffff"><p align="justify" class="inline-table">Время передачи данных</p></td></tr><tr><td style="background-color:#ffffff"><p align="left" class="inline-table">1</p></td><td style="background-color:#ffffff"><p align="left" class="inline-table">Переместить отношение <i>Р </i>на узел <i>А</i></p></td><td style="background-color:#ffffff"> </td></tr><tr><td style="background-color:#ffffff"><p align="left" class="inline-table">2</p></td><td style="background-color:#ffffff"><p align="left" class="inline-table">Переместить отношение <i>S </i>и <i>SP </i>на узел <i>В</i></p></td><td style="background-color:#ffffff"><p align="left" class="inline-table">1,12 ч</p></td></tr><tr><td style="background-color:#ffffff"><p align="left" class="inline-table">3</p></td><td style="background-color:#ffffff"><p align="justify" class="inline-table">Для каждой поставки из Лондона проверить,</p><p align="left" class="inline-table">является ли деталь красной</p></td><td style="background-color:#ffffff"><p align="left" class="inline-table">5,56 ч</p></td></tr><tr><td style="background-color:#ffffff"><p align="left" class="inline-table">4</p></td><td style="background-color:#ffffff"><p align="justify" class="inline-table">Для каждой ли красной детали проверить, не</p><p align="left" class="inline-table">поставляется ли она из Лондона</p></td><td style="background-color:#ffffff"><p align="left" class="inline-table">2 с</p></td></tr><tr><td style="background-color:#ffffff"><p align="left" class="inline-table">5</p></td><td style="background-color:#ffffff"><p align="justify" class="inline-table">Переместить сведения о поставках из Лондона на</p><p align="left" class="inline-table">узел <i>В</i></p></td><td style="background-color:#ffffff"><p align="left" class="inline-table">6,67 мин</p></td></tr><tr><td style="background-color:#ffffff"><p align="left" class="inline-table">6</p></td><td style="background-color:#ffffff"><p align="left" class="inline-table">Переместить сведения о красных деталей</p></td><td style="background-color:#ffffff"><p align="left" class="inline-table">0,1 с (наилучший результат)</p></td></tr></tbody></table></div></div><span id="lnk5a5d198309994d879a38664a8fb59600"> </span><div class="section"><p align="justify" class="paragraph-source">     Внимательно ознакомившись с этими результатами, можно отметить следующие важные особенности:</p><p align="justify" class="paragraph-source">     - Каждая из шести стратегий представляет собой один из возможных подходов к решению этой проблемы, несмотря на очень значительные вариации во времени передачи данных.</p><p align="justify" class="paragraph-source">     - Интенсивность обмена данными и задержка доступа являются важными факторами, влияющими на выбор той или иной стратегии.</p><p align="justify" class="paragraph-source">     - Для плохих стратегий продолжительность операций вычисления и ввода-вывода данных пренебрежимо мала по сравнению со временем передачи данных. </p><p align="justify" class="paragraph-source">     В дополнение к сказанному выше следует отметить, что некоторые стратегии позволяют выполнять параллельную обработку на двух узлах. Таким образом, время отклика в такой системе может оказаться меньше времени отклика в централизованной системе. Обратите внимание, что в данном обсуждении игнорировался вопрос о том, какой узел получает окончательные результаты.</p></div><span id="lnk7440f8dc02b34732b713f1914e783388"> </span><h2 align="left" class="paragraph-headline-source">     Управление каталогом</h2><span id="lnkbebf465f6fea404d9eaa27787e2b6737"> </span><div class="section"><p align="justify" class="paragraph-source">     Каталог распределенной системы содержит не только обычные данные, касающиеся базовых отношений, представлений, индексов, пользователей и т.д., но также и всю информацию, необходимую для обеспечения независимости размещения, фрагментации и репликации. В таком случае возникает вопрос: где и как следует хранить системный каталог? Ниже перечислены некоторые варианты хранения системного каталога.</p><p align="justify" class="paragraph-source">     <i>Централизованный каталог</i>. Весь каталог хранится в одном месте, т.е. на центральном узле.</p><p align="justify" class="paragraph-source">     <i>Полностью реплицированный каталог</i>. Весь каталог полностью хранится на каждом узле.</p><p align="justify" class="paragraph-source">     <i>Секционированный каталог</i>. На каждом узле содержится его собственный каталог для объектов, хранимых на этом узле. Общий каталог является объединением всех разъединенных локальных каталогов.</p><p align="justify" class="paragraph-source">     <i>Комбинация первого и третьего вариантов</i>. На каждом узле хранится собственный локальный каталог (как в п. 3), кроме того, на одном центральном узле хранится унифицированная копия всех этих локальных каталогов (как в п. 1).</p><p align="justify" class="paragraph-source">     Для каждого подхода характерны определенные недостатки и проблемы. В первом подходе, очевидно, не достигается "независимость от центрального узла". Во втором утрачивается автономность функционирования, поскольку при обновлении каждого каталога это обновление придется распространить на каждый узел. В третьем выполнение нелокальных операций становится весьма дорогостоящим (для поиска удаленного объекта потребуется в среднем осуществить доступ к половине имеющихся узлов). Четвертый подход более эффективен, чем третий (для поиска удаленного объекта потребуется осуществить доступ только к одному удаленному каталогу), но в нем снова не достигается "независимость от центрального узла". В традиционных системах могут использовать <i>другие </i>подходы. В качестве примера здесь рассматривается подход, использованный в системе R<i>*</i>.</p><p align="justify" class="paragraph-source">     Для того чтобы описать структуру каталога системы R*, необходимо прежде всего рассмотреть принятый в ней порядок <b>именования </b>объектов. Именование объектов является существенным аспектом распределенных систем, поскольку, если два разных узла <i>А </i>и <i>В </i>содержат хранимое отношение под одинаковым именем <i>R</i>, это приводит к необходимости выработки некоторого метода обеспечения уникальности имен в рамках всей системы. Однако при указании пользователю уточненного имени (например, <i>A</i>.<i>R </i>и <i>B.R</i>) нарушается требование независимости расположения. В таком случае необходимо отображение известных пользователям имен на соответствующие системные имена.</p><p align="justify" class="paragraph-source">     В системе R* используется следующий подход к этой проблеме. В ней вводятся понятия <b>печатного имени</b>, т.е. имени объекта, на которое обычно ссылаются пользователи (например, в выражениях SQL), а также системного имени, которое является глобальным уникальным внутренним идентификатором этого объекта. Системное имя содержит четыре компонента:</p><p align="justify" class="paragraph-source">     - <i>идентификатор создателя </i>объекта;</p><p align="justify" class="paragraph-source">     - <i>идентификатор узла создателя</i>, т.е. узла, на котором была выполнена операция создания объекта;</p><p align="justify" class="paragraph-source">     - <i>локальное имя</i>, т.е. неуточненное имя объекта;</p><p align="justify" class="paragraph-source">     -<i>идентификатор узла хранения</i>, т.е. узла, на котором этот объект хранился в исходном состоянии.</p><p align="justify" class="paragraph-source">     Так, системное имя MARYLIN @ NEW YORK . STATS @ LONDON идентифицирует объект (например, хранимое отношение) с локальным именем STATS, созданный пользователем Marylin на узле в Нью-Йорке и изначально хранившийся на узле в Лондоне. Такое имя <b>гарантировано от каких-либо изменений </b>даже при перемещении этого объекта на другой узел.</p><p align="justify" class="paragraph-source">     Как уже отмечалось, пользователи обычно применяют к объектам их <i>печатные имена</i>, которые состоят из простого неуточненного имени: например, либо "локального компонента" системного имени (STATS в рассматриваемом примере), либо <b>синонима </b>системного имени, определенного с помощью специальной инструкции CREATЕ SYNONYM языка SQL, используемого в системе R<i>*</i>. </p><p align="justify" class="paragraph-source">     Приведем <u>пример </u>такой инструкции.</p><p align="center" class="paragraph-source">CREATЕ SYNONYM MSTATS FOR MARYLIN@NEWYORK.STATS@LONDON;</p><p align="justify" class="paragraph-source">     Теперь можно использовать одно из следующих выражений:</p><p align="justify" class="paragraph-source">SELECT . . . FROM STATS . . . ;</p><p align="justify" class="paragraph-source">SELECT . . . FROM MSTATS . . . ;</p><p align="justify" class="paragraph-source">     В первом случае (при использовании локального имени) системное имя может быть выведено на основе очевидных и принятых по умолчанию предположений, а именно на основе того, что данный объект был создан данным пользователем на данном узле и изначально хранился на этом узле. Одним из следствий такого способа будет то, что старые приложения для System R могут быть без всяких изменений запущены на выполнение в системе R*.</p><p align="justify" class="paragraph-source">     Во втором случае (при использовании синонимов) системное имя определяется помощью опроса соответствующей <b>таблицы синонимов</b>. Эти таблицы рассматриваются как первый компонент каталога, а каждый узел содержит набор таблиц всех пользователей, известных на данном узле, с отображением синонимов пользователя на системные имена.</p><p align="justify" class="paragraph-source">     В дополнение к таблицам синонимов на каждом узле поддерживаются следующие объекты:</p><p align="justify" class="paragraph-source">     - элемент каталога для каждого объекта, <i>созданного </i>на этом узле;</p><p align="justify" class="paragraph-source">     - элемент каталога для каждого объекта, <i>хранимого </i>в данный момент на этом узле.</p><p align="justify" class="paragraph-source">     Допустим, пользователь создает запрос для поиска синонима MSTATS. Сначала система ищет соответствующее ему системное имя в соответствующей таблице синонимов (чисто локальная подстановка). После этого становится известным место создания объекта; в рассматриваемом примере это Лондон. Затем система опрашивает каталог Лондона, что в общем случае приводит к подстановке с удаленным доступом (первый удаленный доступ). Каталог Лондона будет содержать элемент для данного объекта согласно упомянутому выше пункту 1. Если искомый объект находится все еще в Лондоне, то он будет немедленно найден. Однако, если он перемещен, например в Лос-Анджелес, в таком случае это будет указано в элементе каталога Лондона. Благодаря такой организации система теперь может опросить каталог Лос-Анджелеса (второй удаленный доступ), который согласно пункту 2 будет содержать элемент для искомого объекта. Таким образом, этот объект будет найден, по крайней мере, с помощью двух попыток удаленного доступа.</p><p align="justify" class="paragraph-source">     Более того, при очередной миграции объекта, например в Сан-Франциско, в системе будут выполнены следующие действия:</p><p align="justify" class="paragraph-source">     - вставлен элемент каталога Сан-Франциско;</p><p align="justify" class="paragraph-source">     - удален элемент каталога Лос-Анджелеса;</p><p align="justify" class="paragraph-source">     - элемент каталога Лондона, указывающий на Лос-Анджелес, будет заменен элементом каталога, указывающим на Сан-Франциско.</p><p align="justify" class="paragraph-source">     Общий эффект от их выполнения заключается в том, что объект снова может быть найден с помощью всего лишь двух попыток удаленного доступа. К тому же такая система является действительно распределенной, так как в ней нет каталога на центральном узле, а также не существует никакой другой возможности для глобального краха системы.</p><p align="justify" class="paragraph-source">     Следует отметить, что в модуле распределенной работы в системе DB2 используется схема именования объектов, похожая на описанную выше, но не идентичная ей.</p></div><span id="lnk95f3e6cb50a543a8996e57434460de92"> </span><h2 align="left" class="paragraph-headline-source">     Распространение обновления</h2><span id="lnk8648518221264c5d90d69e081f09646e"> </span><div class="section"><p align="justify" class="paragraph-source">     Как указывалось выше, основной проблемой репликации данных является то, что обновление любого логического объекта должно распространяться на все хранимые копии этого объекта. Трудности возникают из-за того, что некоторый узел, содержащий данный объект, может быть недоступен (например, из-за краха системы или данного узла) именно в момент обновления. В таком случае очевидная стратегия немедленного распространения обновлений на все копии может оказаться неприемлемой, поскольку предполагается, что обновление (а значит, и исполнение транзакции) будет провалено, если одна из этих копий будет недоступна в текущий момент. В некотором смысле, при использовании такой стратегии данные действительно будут менее доступны, чем при их использовании в нереплицированном виде. Таким образом, существенно подрывается одно из преимуществ репликации, упомянутое в предыдущем разделе.</p><p align="justify" class="paragraph-source">     Общая схема устранения этой проблемы (и не единственно возможная в этом случае), называемая схемой <b>первичной копии</b>, описана ниже.</p><p align="justify" class="paragraph-source">     - одна копия каждого реплицируемого объекта называется <i>первичной </i>копией, а все остальные - <i>вторичными</i>;</p><p align="justify" class="paragraph-source">     - первичные копии различных объектов находятся на различных узлах (таким образом, эта схема является распределенной);</p><p align="justify" class="paragraph-source">     - операции обновления считаются завершенными, если обновлены все первичные копии. В таком случае <i>в некоторый момент </i>времени узел, содержащий такую копию, несет ответственность за распространение операции обновления на вторичные копии. Однако поскольку свойства транзакции АСИД (атомарность, согласованность, изоляция, долговечность) должны выполняться, то подразумевается, что "некоторый момент" времени предшествует завершению исполнения транзакции.</p><p align="justify" class="paragraph-source">     Конечно, данная схема приводит к некоторым дополнительным проблемам.</p><p align="justify" class="paragraph-source">Обратите внимание, что эти проблемы приводят к нарушению требования локальной автономии, поскольку даже если локальная копия остается доступной, выполнение транзакции может потерпеть неудачу из-за недоступности удаленной (первичной) копии некоторого объекта.</p><p align="justify" class="paragraph-source">     Под требованием атомарности транзакции подразумевается, что распространение всех операций обновления должно быть закончено до завершения соответствующей транзакции. Однако существовало несколько коммерческих программных продуктов с поддержкой менее амбициозной формы репликации. В ней распространение обновления <i>гарантировалось </i>в будущем (вероятно, в некоторое заданное пользователем время), но не обязательно в рамках соответствующей транзакции. К сожалению, термин "репликация" в некоторых программных продуктах был использован в несколько ином смысле, чем следует. В результате на рынке программного обеспечения утвердилось мнение, что распространение обновления откладывается вплоть до завершения соответствующей транзакции. Проблема такого "откладываемого распространения" заключается в том, что пользователь в некоторый заданный момент времени не знает, согласована база данных или нет. Поэтому в базе данных не может быть гарантирована совместимость в произвольный момент времени.</p><p align="justify" class="paragraph-source">     В заключение стоит привести несколько дополнительных замечаний в отношении откладываемого распространения обновления.</p><p align="justify" class="paragraph-source">     Концепция репликации в системе с откладываемым распространением обновления может рассматриваться как ограниченное воплощение идеи снимков.</p><p align="justify" class="paragraph-source">     Одна из причин использования в программных продуктах откладываемого распространения обновления заключается в том, что для обновления всех реплик до завершения транзакции требуется поддержка протокола двухфазной фиксации, для которого, в свою очередь, требуется исправность всех соответствующих узлов и их готовность к запуску во время выполнения транзакции, что отрицательно влияет на производительность. Такое положение дел характеризуется появлением в печати статей с загадочными заголовками типа "Репликация или двухфазная фиксация". Причем их загадочность вызвана тем, что сравниваются преимущества двух совершенно разных подходов.</p></div><span id="lnkf121f7605481450e85b72f4a0ad989db"> </span><h2 align="left" class="paragraph-headline-source">     Управление восстановлением</h2><span id="lnkf17d5f3196af4b66942595211d293915"> </span><div class="section"><p align="justify" class="paragraph-source">     <b>Управление восстановлением </b>в распределенных системах обычно основано на протоколе двухфазной фиксации (или некотором варианте этого протокола). Поддержка двухфазной фиксации необходима в любой среде, в которой одна транзакция может взаимодействовать с несколькими автономными администраторами ресурсов. Однако она особенно важна в распределенной системе, поскольку администраторы ресурсов, т.е. локальные СУБД, действуют на разных узлах и, следовательно, автономны.</p><p align="justify" class="paragraph-source">     Здесь необходимо отметить несколько важных особенностей.</p></div><span id="lnkcf0299dd995b452a87dd3b7d195b8b18"> </span><div id="see-morecf0299dd995b452a87dd3b7d195b8b18" class="additional"><table border="0"><tr><td><img src="../../00/styles/files/seecollapsed.gif" alt="Смотрите также:" title="Смотрите также:" class="collapse-img" style="width:16px; height:16px;" /></td><td><a href="javascript:seeMore('cf0299dd995b452a87dd3b7d195b8b18');" class="paragraph-source">Смотрите также:</a></td></tr></table><div style="display:none;"><p align="justify" class="note-source">     - Стремление к "независимости от центрального узла" означает, что функция координатора не должна присваиваться ни одному из узлов сети, вместо этого она должна выполняться для разных транзакций различными узлами. Обычно она выполняется узлом, на котором данная транзакция была запущена. Таким образом, каждый узел должен для одних транзакций выполнять роль узла-координатора, а для других - узла-участника.</p><p align="justify" class="note-source">     - При двухфазной фиксации требуется, чтобы координатор обменивался данными с каждым узлом-участником, что, в свою очередь, означает большее количество сообщений и больше накладных расходов.</p><p align="justify" class="note-source">     - Если узел Y действует как участник процесса двухфазной фиксации, координируемого узлом X, то узел Y должен выполнять любые действия, предписываемые узлом X (например, завершение или отмену выполнения транзакции). При этом неизбежна утрата локальной автономности.</p><p align="justify" class="note-source">     - В идеальной ситуации, конечно, хотелось бы, чтобы процесс двухфазной фиксации продолжался несмотря на неисправность сети или отдельных узлов либо был устойчив по отношению к любым возможным видам неисправности. К сожалению, легко заметить, что эта проблема неразрешима в принципе, т.е. не существует никакого конечного протокола, который мог бы гарантировать, что все агенты одновременно завершат выполнение успешной транзакции либо отменят выполнение неуспешной транзакции несмотря на неисправности произвольного характера. Можно предположить и обратную ситуацию, т.е. что такой протокол все-таки существует. Пусть минимальное количество сообщений, необходимых для осуществления этого протокола, равняется N. Предположим теперь, что последнее из этих N сообщений утеряно по какой-то причине. Тогда либо это сообщение не является необходимым, что противоречит исходному предположению о минимально необходимом количестве сообщений N, либо такой протокол не будет работать. Любой из этих случаев ведет к противоречию, из чего можно заключить, что такого протокола не существует.</p><p align="justify" class="note-source">     - Протокол двухфазной фиксации может рассматриваться как основной протокол. Однако существует множество модификаций этого основного протокола, которые можно и следовало бы применить на практике. Например, в варианте под названием протокол предполагаемой фиксации, в котором при успешном выполнении транзакции число сообщений уменьшается за счет введения дополнительных сообщений для случая неуспешного выполнения транзакции.</p></div></div><span id="lnke63846d5d3184633a900665a722faebf"> </span><h2 align="left" class="paragraph-headline-source">     Управление параллелизмом</h2><span id="lnkf5767f52cd8d4adeaacc16414a963924"> </span><div class="section"><p align="justify" class="paragraph-source">     Управление параллелизмом в большинстве распределенных систем, как и во многих нераспределенных системах, основано на блокировке. Однако в распределенной системе запросы на проверку, установку и снятие блокировок являются <i>сообщениями </i>(здесь предполагается, что рассматриваемый объект находится на удаленном узле), что влечет за собой дополнительные накладные расходы. Рассмотрим, например, транзакцию <i>Т</i>, которая нуждается в обновлении объекта, имеющего реплики на <i>n </i>удаленных узлах. Если каждый узел управляет блокировками для объектов, хранимых на этом узле (как это было бы при соблюдении локальной автономии), то для простейшего способа управления параллелизмом потребовалось бы по крайней мере 5<i>n </i>сообщений: </p><p align="justify" class="paragraph-source">     <i>-n </i>запросов на блокировку;</p><p align="justify" class="paragraph-source">     -<i>n </i>разрешений на блокировку;</p><p align="justify" class="paragraph-source">     <i>-n </i>сообщений об обновлении;</p><p align="justify" class="paragraph-source">     <i>-n </i>подтверждений;</p><p align="justify" class="paragraph-source">     <i>-n </i>запросов на снятие блокировки.</p><p align="justify" class="paragraph-source">     Конечно, можно легко усовершенствовать этот способ, используя подтверждения, вложенные в блок данных обратного направления. Таким образом могут комбинироваться запрос на блокировку и сообщения об обновлении, а также разрешение на блокировку и подтверждения. Но даже в таком случае общее время обновления может быть на несколько порядков выше, чем в централизованной системе.</p><p align="justify" class="paragraph-source">     Обычный подход к этой проблеме заключается в принятии стратегии <i>первичной копии</i>, которая в краткой форме была представлена выше. Для данного объекта <i>R </i>узел, содержащий первичную копию объекта <i>R</i>, будет управлять всеми операциями блокировки, включающими <i>R </i>(помните, что первичные копии разных объектов в общем случае могут быть расположены на различных узлах). При такой стратегии множество всех копий объекта в целях блокировки может рассматриваться как единый объект, а общее число сообщений будет снижено от 5<i>n </i>до 2<i>n</i>+3 (один запрос на блокировку, одно разрешение на блокировку, <i>n </i>обновлений, <i>n </i>подтверждений и один запрос на снятие блокировки). Однако при таком решении опять утрачивается автономность, т.е. транзакция может оказаться неуспешной, если первичная копия недоступна (даже при использовании транзакции только для чтения), а локальная копия доступна. Заметьте, что для блокировки первичной копии необходимы не только операции обновления, но также и операции извлечения. Таким образом, неприятный побочный эффект при использовании стратегии первичной копии заключается в снижении производительности и доступности данных как для операций извлечения, так и для операций обновления.</p><p align="justify" class="paragraph-source">     Другой проблемой блокировки в распределенной системе является то, что она может привести к <b>глобальному тупику</b>, который охватывает два или более узлов. На рис. 4 представлен пример возникновения такого тупика:</p></div><span id="lnk2b05d18a37a64c79ac03fc167cfe2a04"> </span><div class="section"><p align="center" class="paragraph-source"><img src="../res/files/jpged3_0_.jpg" height="313px" width="543px" alt="" /> </p><p align="center" class="paragraph-source">Рисунок 4 - Пример глобального тупика</p></div><span id="lnka1cb5ef19297412b8bae44ec15af8518"> </span><div id="see-morea1cb5ef19297412b8bae44ec15af8518" class="additional"><table border="0"><tr><td><img src="../../00/styles/files/seecollapsed.gif" alt="Смотрите также:" title="Смотрите также:" class="collapse-img" style="width:16px; height:16px;" /></td><td><a href="javascript:seeMore('a1cb5ef19297412b8bae44ec15af8518');" class="paragraph-source">Смотрите также:</a></td></tr></table><div style="display:none;"><p align="justify" class="note-source">     1. Агент транзакции T2 на узле X ожидает, когда агент транзакции T1 снимет блокировку на узле X</p><p align="justify" class="note-source">     2. Агент транзакции T1 на узле X ожидает, когда закончится выполнение транзакции T1 на узле Y</p><p align="justify" class="note-source">     3. Агент транзакции T1 на узле Y ожидает, когда агент транзакции T2 снимет блокировку на узле Y.</p><p align="justify" class="note-source">     4. Агент транзакции T2 на узле Y ожидает, когда закончится выполнение транзакции</p><p align="justify" class="note-source">T2 на узле X. Тупиковая ситуация!</p><p align="justify" class="note-source">     Проблема тупика такого типа состоит в том, что <i>ни один из узлов не может обнаружить тупик, используя только информацию, которая сосредоточена на этом узле</i><i>.</i></p><p align="justify" class="note-source">     Иначе говоря, в локальных диаграммах ожидания нет никаких циклов, но они появятся при объединении локальных диаграмм в глобальную диаграмму ожидания. Отсюда следует, что обнаружение глобальных тупиков связано с увеличением накладных расходов, поскольку для этого требуется дополнительно совместить отдельные локальные диаграммы.</p></div></div></div></td></tr></table></td></tr><tr><td id="bottomOuter"><table id="bottomInner"><tr><td class="cell1"><div> </div></td><td class="cell2"><div>© 2012 ХНУРЭ, ПИ, Широкопетлева М.С., <a href="mailto:mshirokopetleva@gmail.com">mshirokopetleva@gmail.com</a>;   ХНУРЭ, ПИ, Черепанова Ю.Ю.<br /><a href="http://lersus.de/" title="Данный курс создан с помощью авторской системы LERSUS" target="_blank">Разработано с помощью LERSUS</a></div></td></tr></table></td></tr></table></body></html>